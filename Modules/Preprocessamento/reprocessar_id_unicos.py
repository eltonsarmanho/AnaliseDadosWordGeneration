#!/usr/bin/env python3
"""
Reprocessador de ID_Ãšnicos para Datasets Consolidados
====================================================

ApÃ³s as correÃ§Ãµes nos nomes das escolas, alguns ID_Ãºnicos ficaram inconsistentes.
Este script reprocessa e corrige todos os ID_Ãºnicos nos datasets consolidados.

Formato do ID_Ãšnico:
NOME_ESCOLA_TURMA_FASE

Exemplo:
JOÃƒO SILVA_EMEB PADRE ANCHIETA_7Â° ANO A_F2
"""

import pandas as pd
import re
import unicodedata
from pathlib import Path

def normalizar_string_para_id(texto: str) -> str:
    """Normaliza string para uso em ID_Ãšnico (remove acentos, espaÃ§os extras, etc.)"""
    if pd.isna(texto) or texto == "":
        return ""
    
    # Converter para string e normalizar
    texto = str(texto).strip()
    
    # Remover acentos
    texto = unicodedata.normalize('NFD', texto)
    texto = ''.join(c for c in texto if unicodedata.category(c) != 'Mn')
    
    # Converter para maiÃºsculo
    texto = texto.upper()
    
    # Substituir caracteres especiais por espaÃ§o
    texto = re.sub(r'[^\w\s]', ' ', texto)
    
    # Remover espaÃ§os mÃºltiplos
    texto = re.sub(r'\s+', ' ', texto)
    
    return texto.strip()

def gerar_id_unico(nome: str, escola: str, turma: str, fase: str) -> str:
    """Gera um ID_Ãšnico padronizado"""
    
    # Normalizar cada componente
    nome_norm = normalizar_string_para_id(nome)
    escola_norm = normalizar_string_para_id(escola)
    turma_norm = normalizar_string_para_id(turma)
    fase_norm = str(fase).strip()
    
    # Validar componentes
    if not nome_norm or not escola_norm or not turma_norm or not fase_norm:
        return ""
    
    # Construir ID_Ãšnico
    id_unico = f"{nome_norm}_{escola_norm}_{turma_norm}_F{fase_norm}"
    
    return id_unico

def reprocessar_dataset(arquivo_path: str, backup: bool = True) -> pd.DataFrame:
    """Reprocessa um dataset corrigindo os ID_Ãºnicos"""
    
    print(f"ğŸ”§ Reprocessando: {Path(arquivo_path).name}")
    
    # Carregar dataset
    df = pd.read_csv(arquivo_path)
    
    # Fazer backup se solicitado
    if backup:
        backup_path = f"{arquivo_path}.backup_id_reprocessado"
        df.to_csv(backup_path, index=False)
        print(f"   ğŸ’¾ Backup criado: {Path(backup_path).name}")
    
    # Verificar colunas necessÃ¡rias
    colunas_necessarias = ['Nome', 'Escola', 'Turma', 'Fase']
    colunas_faltantes = [col for col in colunas_necessarias if col not in df.columns]
    
    if colunas_faltantes:
        print(f"   âŒ Colunas faltantes: {colunas_faltantes}")
        return df
    
    # Gerar novos ID_Ãºnicos
    print(f"   ğŸ”„ Gerando novos ID_Ãºnicos...")
    novos_ids = []
    ids_problematicos = 0
    
    for idx, row in df.iterrows():
        novo_id = gerar_id_unico(
            nome=row['Nome'],
            escola=row['Escola'], 
            turma=row['Turma'],
            fase=row['Fase']
        )
        
        if novo_id == "":
            ids_problematicos += 1
            # Manter ID original se nÃ£o conseguir gerar novo
            novo_id = row.get('ID_Unico', f"PROBLEMA_LINHA_{idx}")
        
        novos_ids.append(novo_id)
    
    # Atualizar coluna ID_Unico
    df['ID_Unico'] = novos_ids
    
    # EstatÃ­sticas
    ids_unicos_count = df['ID_Unico'].nunique()
    total_registros = len(df)
    
    print(f"   ğŸ“Š EstatÃ­sticas:")
    print(f"      â€¢ Total de registros: {total_registros:,}")
    print(f"      â€¢ ID_Ãºnicos gerados: {ids_unicos_count:,}")
    print(f"      â€¢ IDs problemÃ¡ticos: {ids_problematicos}")
    
    if ids_problematicos > 0:
        print(f"   âš ï¸  {ids_problematicos} IDs problemÃ¡ticos encontrados")
    
    # Verificar duplicaÃ§Ãµes
    duplicados = df[df.duplicated(subset=['ID_Unico'], keep=False)]
    if not duplicados.empty:
        print(f"   âš ï¸  {len(duplicados)} registros com ID_Ãºnicos duplicados")
        
        # Mostrar alguns exemplos
        ids_duplicados = duplicados['ID_Unico'].value_counts().head(3)
        print(f"      Exemplos de duplicados:")
        for id_dup, count in ids_duplicados.items():
            print(f"         â€¢ {id_dup}: {count} ocorrÃªncias")
    
    return df

def validar_id_unicos(df: pd.DataFrame, nome_dataset: str):
    """Valida a qualidade dos ID_Ãºnicos gerados"""
    
    print(f"\nğŸ” VALIDAÃ‡ÃƒO: {nome_dataset}")
    print("-" * 30)
    
    # EstatÃ­sticas bÃ¡sicas
    total = len(df)
    unicos = df['ID_Unico'].nunique()
    duplicados = total - unicos
    
    print(f"ğŸ“Š Registros totais: {total:,}")
    print(f"ğŸ“Š ID_Ãºnicos: {unicos:,}")
    print(f"ğŸ“Š Duplicados: {duplicados}")
    
    if duplicados > 0:
        print(f"âš ï¸  Taxa de duplicaÃ§Ã£o: {(duplicados/total)*100:.2f}%")
    else:
        print(f"âœ… Sem duplicaÃ§Ãµes!")
    
    # Verificar padrÃµes dos IDs
    ids_sample = df['ID_Unico'].dropna().head(5).tolist()
    print(f"\nğŸ“ Exemplos de ID_Ãºnicos:")
    for i, id_exemplo in enumerate(ids_sample, 1):
        print(f"   {i}. {id_exemplo}")

def main():
    """FunÃ§Ã£o principal de reprocessamento"""
    
    print("ğŸš€ REPROCESSAMENTO DE ID_ÃšNICOS")
    print("=" * 60)
    
    # Caminhos dos datasets consolidados
    dashboard_path = Path("/home/nees/Documents/VSCodigo/AnaliseDadosWordGeneration/Dashboard")
    
    datasets = [
        dashboard_path / "TDE_consolidado_fases_2_3_4.csv",
        dashboard_path / "vocabulario_consolidado_fases_2_3_4.csv"
    ]
    
    # Verificar se arquivos existem
    datasets_existentes = [d for d in datasets if d.exists()]
    
    if not datasets_existentes:
        print("âŒ Nenhum dataset consolidado encontrado!")
        return
    
    # Reprocessar cada dataset
    datasets_processados = {}
    
    for dataset_path in datasets_existentes:
        df_reprocessado = reprocessar_dataset(str(dataset_path))
        datasets_processados[dataset_path.name] = df_reprocessado
        
        # Salvar dataset reprocessado
        df_reprocessado.to_csv(str(dataset_path), index=False)
        print(f"   âœ… Dataset reprocessado salvo\n")
    
    # ValidaÃ§Ã£o final
    print("ğŸ” VALIDAÃ‡ÃƒO FINAL")
    print("=" * 30)
    
    for nome_dataset, df in datasets_processados.items():
        validar_id_unicos(df, nome_dataset)
    
    # Verificar consistÃªncia entre datasets
    if len(datasets_processados) > 1:
        print(f"\nğŸ”„ VERIFICAÃ‡ÃƒO DE CONSISTÃŠNCIA")
        print("-" * 35)
        
        # Comparar estrutura de IDs entre datasets
        datasets_list = list(datasets_processados.items())
        for i in range(len(datasets_list) - 1):
            nome1, df1 = datasets_list[i]
            nome2, df2 = datasets_list[i + 1]
            
            ids_comuns = set(df1['ID_Unico']) & set(df2['ID_Unico'])
            print(f"ğŸ“Š IDs em comum entre {nome1} e {nome2}: {len(ids_comuns):,}")
    
    print(f"\nâœ… REPROCESSAMENTO CONCLUÃDO!")
    print(f"ğŸ“‹ Arquivos processados: {len(datasets_processados)}")
    print(f"ğŸ’¾ Backups criados para seguranÃ§a")
    print(f"ğŸ¯ ID_Ãºnicos padronizados e corrigidos")

if __name__ == "__main__":
    main()