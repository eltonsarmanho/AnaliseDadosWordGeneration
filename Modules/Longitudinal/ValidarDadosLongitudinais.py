#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VALIDA√á√ÉO DE DADOS LONGITUDINAIS - WORDGEN
Script para validar a qualidade dos dados longitudinais das Fases 2, 3 e 4

Verifica:
1. Dados duplicados dentro de cada CSV (nome, escola, turma iguais)
2. Integridade dos dados por fase
3. Consist√™ncia entre TDE e Vocabul√°rio

Autor: Sistema de An√°lise WordGen
Data: 2024
"""

import pandas as pd
import pathlib
from typing import Dict, List, Tuple
import json

# ======================
# Configura√ß√µes de Paths
# ======================
BASE_DIR = pathlib.Path(__file__).parent.parent.parent.resolve()
DATA_DIR = BASE_DIR / "Data"
LONGITUDINAL_DIR = BASE_DIR / "Modules" / "Longitudinal" / "Data"

# Arquivos longitudinais consolidados
CSV_LONGITUDINAL_TDE = LONGITUDINAL_DIR / "dados_longitudinais_TDE.csv"
CSV_LONGITUDINAL_VOCAB = LONGITUDINAL_DIR / "dados_longitudinais_Vocabulario.csv"

# Arquivos originais por fase
FASES_ORIGINAIS = {
    'Fase 2': {
        'TDE': {
            'Pre': DATA_DIR / "Fase 2" / "Pre" / "DadosTDE.csv",
            'Pos': DATA_DIR / "Fase 2" / "Pos" / "DadosTDE.csv"
        },
        'Vocabulario': {
            'Pre': DATA_DIR / "Fase 2" / "Pre" / "DadosVocabulario.csv", 
            'Pos': DATA_DIR / "Fase 2" / "Pos" / "DadosVocabulario.csv"
        }
    },
    'Fase 3': {
        'TDE': {
            'Pre': DATA_DIR / "Fase 3" / "Pre" / "DadosTDE.csv",
            'Pos': DATA_DIR / "Fase 3" / "Pos" / "DadosTDE.csv"
        },
        'Vocabulario': {
            'Pre': DATA_DIR / "Fase 3" / "Pre" / "DadosVocabulario.csv",
            'Pos': DATA_DIR / "Fase 3" / "Pos" / "DadosVocabulario.csv"
        }
    },
    'Fase 4': {
        'TDE': {
            'Pre': DATA_DIR / "Fase 4" / "Pre" / "DadosTDE.csv",
            'Pos': DATA_DIR / "Fase 4" / "Pos" / "DadosVocabulario.csv"  # Nota: arquivo tem nome incorreto
        },
        'Vocabulario': {
            'Pre': DATA_DIR / "Fase 4" / "Pre" / "DadosVocabulario.csv",
            'Pos': DATA_DIR / "Fase 4" / "Pos" / "DadosTDE.csv"  # Nota: arquivo tem nome incorreto
        }
    }
}

def normalizar_nomes_colunas(df):
    """Normaliza nomes das colunas para padr√£o consistente"""
    # Mapeamento de varia√ß√µes de nomes
    mapeamento = {
        # Varia√ß√µes de Escola
        'escola': 'Escola',
        'ESCOLA': 'Escola',
        'School': 'Escola',
        
        # Varia√ß√µes de Turma
        'turma': 'Turma',
        'TURMA': 'Turma',
        'Class': 'Turma',
        
        # Varia√ß√µes de Nome
        'nome': 'Nome',
        'NOME': 'Nome',
        'Name': 'Nome',
        
        # Varia√ß√µes de Sexo
        'sexo': 'Sexo',
        'SEXO': 'Sexo',
        'Gender': 'Sexo',
        
        # Varia√ß√µes de quest√µes
        'Q1': 'P1', 'Q2': 'P2', 'Q3': 'P3', 'Q4': 'P4',
        'Q5': 'P5', 'Q6': 'P6', 'Q7': 'P7', 'Q8': 'P8'
    }
    
    # Renomear colunas conforme mapeamento
    df_normalizado = df.rename(columns=mapeamento)
    return df_normalizado

def verificar_duplicados_csv(caminho_arquivo: pathlib.Path) -> Dict:
    """
    Verifica duplicados em um arquivo CSV baseado em nome, escola e turma
    
    Returns:
        Dict com informa√ß√µes sobre duplicados encontrados
    """
    resultado = {
        'arquivo': str(caminho_arquivo),
        'existe': caminho_arquivo.exists(),
        'total_registros': 0,
        'duplicados_encontrados': 0,
        'registros_duplicados': [],
        'grupos_duplicados': [],
        'erro': None
    }
    
    if not caminho_arquivo.exists():
        resultado['erro'] = "Arquivo n√£o encontrado"
        return resultado
    
    try:
        # Carregar CSV
        df = pd.read_csv(caminho_arquivo, encoding='utf-8')
        df = normalizar_nomes_colunas(df)
        
        resultado['total_registros'] = len(df)
        
        # Verificar se existem as colunas necess√°rias
        colunas_necessarias = ['Nome', 'Escola', 'Turma']
        colunas_existentes = [col for col in colunas_necessarias if col in df.columns]
        
        if len(colunas_existentes) < 3:
            resultado['erro'] = f"Colunas necess√°rias n√£o encontradas. Dispon√≠veis: {list(df.columns)}"
            return resultado
        
        # Limpar dados vazios/nulos para an√°lise de duplicados
        df_limpo = df.dropna(subset=colunas_necessarias)
        
        # Identificar duplicados baseado em Nome, Escola e Turma
        duplicados_mask = df_limpo.duplicated(subset=colunas_necessarias, keep=False)
        duplicados_df = df_limpo[duplicados_mask]
        
        resultado['duplicados_encontrados'] = len(duplicados_df)
        
        if len(duplicados_df) > 0:
            # Agrupar duplicados
            grupos = duplicados_df.groupby(colunas_necessarias)
            
            for grupo_key, grupo_df in grupos:
                nome, escola, turma = grupo_key
                grupo_info = {
                    'nome': nome,
                    'escola': escola, 
                    'turma': turma,
                    'num_ocorrencias': len(grupo_df),
                    'indices': grupo_df.index.tolist()
                }
                resultado['grupos_duplicados'].append(grupo_info)
                
                # Adicionar registros individuais
                for idx, row in grupo_df.iterrows():
                    registro = {
                        'indice': idx,
                        'nome': row['Nome'],
                        'escola': row['Escola'],
                        'turma': row['Turma']
                    }
                    resultado['registros_duplicados'].append(registro)
        
    except Exception as e:
        resultado['erro'] = str(e)
    
    return resultado

def validar_arquivos_longitudinais():
    """Valida arquivos longitudinais consolidados"""
    print("üîç VALIDA√á√ÉO DE DADOS LONGITUDINAIS - WORDGEN")
    print("=" * 60)
    
    arquivos_longitudinais = [
        ('TDE Longitudinal', CSV_LONGITUDINAL_TDE),
        ('Vocabul√°rio Longitudinal', CSV_LONGITUDINAL_VOCAB)
    ]
    
    resultados_longitudinais = {}
    
    for nome, arquivo in arquivos_longitudinais:
        print(f"\nüìä Validando: {nome}")
        print("-" * 40)
        
        resultado = verificar_duplicados_csv(arquivo)
        resultados_longitudinais[nome] = resultado
        
        if resultado['erro']:
            print(f"‚ùå ERRO: {resultado['erro']}")
            continue
            
        print(f"üìÅ Arquivo: {resultado['arquivo']}")
        print(f"üìà Total de registros: {resultado['total_registros']:,}")
        print(f"üîç Duplicados encontrados: {resultado['duplicados_encontrados']:,}")
        
        if resultado['duplicados_encontrados'] > 0:
            print(f"‚ö†Ô∏è  ATEN√á√ÉO: Encontrados {resultado['duplicados_encontrados']} registros duplicados!")
            print(f"üî¢ Grupos de duplica√ß√£o: {len(resultado['grupos_duplicados'])}")
            
            print("\nüìã Detalhes dos grupos duplicados:")
            for i, grupo in enumerate(resultado['grupos_duplicados'][:5], 1):  # Mostrar apenas 5 primeiros
                print(f"  {i}. {grupo['nome']} | {grupo['escola']} | {grupo['turma']} ({grupo['num_ocorrencias']} ocorr√™ncias)")
            
            if len(resultado['grupos_duplicados']) > 5:
                print(f"  ... e mais {len(resultado['grupos_duplicados']) - 5} grupos")
        else:
            print("‚úÖ Nenhum duplicado encontrado!")
    
    return resultados_longitudinais

def validar_arquivos_originais():
    """Valida arquivos originais por fase"""
    print(f"\nüîç VALIDA√á√ÉO DE ARQUIVOS ORIGINAIS POR FASE")
    print("=" * 60)
    
    resultados_originais = {}
    
    for fase, tipos in FASES_ORIGINAIS.items():
        print(f"\nüìÖ {fase}")
        print("-" * 30)
        
        resultados_originais[fase] = {}
        
        for tipo, momentos in tipos.items():
            print(f"\n  üìä {tipo}")
            
            resultados_originais[fase][tipo] = {}
            
            for momento, arquivo in momentos.items():
                print(f"    üìÅ {momento}: ", end="")
                
                resultado = verificar_duplicados_csv(arquivo)
                resultados_originais[fase][tipo][momento] = resultado
                
                if resultado['erro']:
                    print(f"‚ùå {resultado['erro']}")
                    continue
                
                status = "‚úÖ OK" if resultado['duplicados_encontrados'] == 0 else f"‚ö†Ô∏è  {resultado['duplicados_encontrados']} duplicados"
                print(f"{status} ({resultado['total_registros']} registros)")
                
                if resultado['duplicados_encontrados'] > 0:
                    print(f"      üîç Grupos duplicados: {len(resultado['grupos_duplicados'])}")
    
    return resultados_originais

def gerar_relatorio_validacao(resultados_longitudinais, resultados_originais):
    """Gera relat√≥rio de valida√ß√£o em JSON"""
    relatorio = {
        'timestamp': pd.Timestamp.now().isoformat(),
        'validacao_longitudinal': resultados_longitudinais,
        'validacao_original': resultados_originais,
        'resumo': {
            'total_arquivos_validados': 0,
            'arquivos_com_duplicados': 0,
            'total_duplicados_encontrados': 0
        }
    }
    
    # Calcular resumo
    total_arquivos = 0
    arquivos_com_duplicados = 0
    total_duplicados = 0
    
    # Arquivos longitudinais
    for nome, resultado in resultados_longitudinais.items():
        if not resultado['erro']:
            total_arquivos += 1
            if resultado['duplicados_encontrados'] > 0:
                arquivos_com_duplicados += 1
                total_duplicados += resultado['duplicados_encontrados']
    
    # Arquivos originais
    for fase, tipos in resultados_originais.items():
        for tipo, momentos in tipos.items():
            for momento, resultado in momentos.items():
                if not resultado['erro']:
                    total_arquivos += 1
                    if resultado['duplicados_encontrados'] > 0:
                        arquivos_com_duplicados += 1
                        total_duplicados += resultado['duplicados_encontrados']
    
    relatorio['resumo'] = {
        'total_arquivos_validados': total_arquivos,
        'arquivos_com_duplicados': arquivos_com_duplicados,
        'total_duplicados_encontrados': total_duplicados,
        'porcentagem_arquivos_com_duplicados': (arquivos_com_duplicados / total_arquivos * 100) if total_arquivos > 0 else 0
    }
    
    # Salvar relat√≥rio
    arquivo_relatorio = LONGITUDINAL_DIR / "relatorio_validacao_duplicados.json"
    with open(arquivo_relatorio, 'w', encoding='utf-8') as f:
        json.dump(relatorio, f, ensure_ascii=False, indent=2)
    
    print(f"\nüíæ Relat√≥rio salvo em: {arquivo_relatorio}")
    
    return relatorio

def main():
    """Fun√ß√£o principal de valida√ß√£o"""
    print("üöÄ INICIANDO VALIDA√á√ÉO DE DADOS LONGITUDINAIS")
    print("=" * 70)
    
    # Validar arquivos longitudinais consolidados
    resultados_longitudinais = validar_arquivos_longitudinais()
    
    # Validar arquivos originais por fase
    resultados_originais = validar_arquivos_originais()
    
    # Gerar relat√≥rio
    relatorio = gerar_relatorio_validacao(resultados_longitudinais, resultados_originais)
    
    # Resumo final
    print(f"\nüìä RESUMO DA VALIDA√á√ÉO")
    print("=" * 30)
    print(f"üìÅ Total de arquivos validados: {relatorio['resumo']['total_arquivos_validados']}")
    print(f"‚ö†Ô∏è  Arquivos com duplicados: {relatorio['resumo']['arquivos_com_duplicados']}")
    print(f"üîç Total de duplicados encontrados: {relatorio['resumo']['total_duplicados_encontrados']}")
    print(f"üìà Porcentagem com duplicados: {relatorio['resumo']['porcentagem_arquivos_com_duplicados']:.1f}%")
    
    if relatorio['resumo']['total_duplicados_encontrados'] == 0:
        print("\n‚úÖ VALIDA√á√ÉO CONCLU√çDA: Nenhum duplicado encontrado!")
    else:
        print("\n‚ö†Ô∏è  VALIDA√á√ÉO CONCLU√çDA: Duplicados encontrados - revisar dados!")
    
    return relatorio

if __name__ == "__main__":
    main()